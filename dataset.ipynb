{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'datasets/flowers'\n",
    "save_dir = 'datasets/saved'\n",
    "save_file = os.path.join(save_dir, 'img-features-{0}x{0}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base `Dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, data_dir, **kwargs):\n",
    "        self._data_dir = data_dir\n",
    "        # Keyword arguments\n",
    "        self._logging = kwargs['logging'] if 'logging' in kwargs else True\n",
    "        \n",
    "    \n",
    "    def save(self, save_file, force=False):\n",
    "        \"\"\"Saves the dataset object.\"\"\"\n",
    "        if os.path.isfile(save_file) and not force:\n",
    "            raise FileExistsError('{} already exist. Set `force=True` to override.'.format(save_file))\n",
    "        dirs = save_file.split('/')\n",
    "        if len(dirs) > 1 and not os.path.isdir('/'.join(dirs[:-1])):\n",
    "            os.makedirs('/'.join(dirs[:-1]))\n",
    "        with open(save_file, 'wb') as f:\n",
    "            pickle.dump(obj=self, file=f)\n",
    "\n",
    "    def load(self, save_file):\n",
    "        if not os.path.isfile(save_file):\n",
    "            raise FileNotFoundError('{} was not found.'.format(save_file))\n",
    "        with open(save_file, 'rb') as f:\n",
    "            self = pickle.load(file=f)\n",
    "        return self\n",
    "    \n",
    "    def next_batch(self, batch_size, shuffle=True):\n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            permute = np.arange(self._num_examples)\n",
    "            np.random.shuffle(permute)\n",
    "            self._X = self._X[permute]\n",
    "            self._y = self._y[permute]\n",
    "        # Go to next batch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_examples = self._num_examples - start\n",
    "            rest_features = self._X[start:self._num_examples]\n",
    "            rest_labels = self._y[start:self._num_examples]\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                permute = np.arange(self._num_examples)\n",
    "                np.random.shuffle(permute)\n",
    "                self._X = self._X[permute]\n",
    "                self._y = self._y[permute]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_examples\n",
    "            end = self._index_in_epoch\n",
    "            features = np.concatenate((rest_features, self._X[start:end]), axis=0)\n",
    "            labels = np.concatenate((rest_labels, self._y[start:end]), axis=0)\n",
    "            return features, labels\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._X[start:end], self._y[start:end]\n",
    "    \n",
    "    def train_test_split(self, test_size=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Splits dataset into training and testing set.\n",
    "\n",
    "        :param dataset: Dataset to be split.\n",
    "        :param test_size: float, default 0.1\n",
    "                    Size of the testing data in %.\n",
    "                    Default is 0.1 or 10% of the dataset.\n",
    "        :keyword valid_portion: float\n",
    "                    Size of validation set in %.\n",
    "                    This will be taking from training set\n",
    "                    after splitting into training and testing set.\n",
    "        :return: np.array of train_X, train_y, test_X, test_y\n",
    "        \"\"\"\n",
    "        test_size = int(len(self._X) * test_size)\n",
    "\n",
    "        train_X = self._X[:-test_size]\n",
    "        train_y = self._y[:-test_size]\n",
    "        test_X = self._X[-test_size:]\n",
    "        test_y = self._y[-test_size:]\n",
    "\n",
    "        if 'valid_portion' in kwargs:\n",
    "            valid_portion = kwargs['valid_portion']\n",
    "            valid_portion = int(len(train_X) * valid_portion)\n",
    "\n",
    "            train_X = train_X[:-valid_portion]\n",
    "            train_y = train_y[:-valid_portion]\n",
    "            val_X = train_X[-valid_portion:]\n",
    "            val_y = train_y[-valid_portion:]\n",
    "            return np.array([train_X, train_y, test_X, test_y, val_X, val_y])\n",
    "\n",
    "        return np.array([train_X, train_y, test_X, test_y])\n",
    "    \n",
    "    @property\n",
    "    def features(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._y\n",
    "    \n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self._labels)\n",
    "    \n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "    \n",
    "    def _create_label(self, label):\n",
    "        hot = np.zeros(shape=[len(self._labels)], dtype=int)\n",
    "        hot[self._labels.index(label)] = 1\n",
    "        return hot\n",
    "        \n",
    "    \n",
    "    def _one_hot(self, arr):\n",
    "        arr, uniques = list(arr), list(set(arr))\n",
    "        encoding = np.zeros(shape=[len(arr), len(uniques)], dtype=np.int32)\n",
    "        for i, a in enumerate(arr):\n",
    "            encoding[i, uniques.index(a)] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ImageDataset` class for image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, grayscale=False, flatten=False, size=50, **kwargs):\n",
    "        super().__init__(data_dir, **kwargs)\n",
    "        self.grayscale = grayscale\n",
    "        self.flatten = flatten\n",
    "        self.size = size\n",
    "        self._labels = [l for l in os.listdir(self._data_dir) if l[0] is not '.']\n",
    "    \n",
    "    def create(self):\n",
    "        self._process()\n",
    "        self._num_examples = self._X.shape[0]\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        \n",
    "    def _process(self):\n",
    "        datasets = []\n",
    "        for i, label in enumerate(self._labels):\n",
    "            image_dir = os.path.join(self._data_dir, label)\n",
    "            image_list = [d for d in os.listdir(image_dir) if d[0] is not '.']\n",
    "            for j, file in enumerate(image_list):\n",
    "                try:\n",
    "                    path = os.path.join(image_dir, file)\n",
    "                    img = Image.open(path)\n",
    "                    img = img.resize((self.size, self.size))\n",
    "                    if self.grayscale:\n",
    "                        img = img.convert('L')\n",
    "                    img = np.array(img, dtype=np.float32)\n",
    "                    if self.flatten:\n",
    "                        img = img.flatten()\n",
    "                    datasets.append([img, self._create_label(label)])\n",
    "                except Exception as e:\n",
    "                    sys.stderr.write('{}'.format(e))\n",
    "                    sys.stderr.flush()\n",
    "                if self._logging:\n",
    "                    sys.stdout.write('\\rProcessing {} of {} class labels & {} of {} images'.format(\n",
    "                    i+1, len(self._labels), j+1, len(image_list)))\n",
    "        # dataset into features & labels\n",
    "        datasets = np.asarray(datasets)\n",
    "        np.random.shuffle(datasets)\n",
    "        self._X = np.array([img for img in datasets[:,0]])\n",
    "        self._y = np.array([label for label in datasets[:,1]])\n",
    "    \n",
    "    @property\n",
    "    def channels(self):\n",
    "        return 1 if len(self._X[0].shape) == 2 else self.X[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TextDataset` for textual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 of 5 class labels & 799 of 799 images\n",
      "Train: X(2643, 50, 50, 3)\tTest: y(734, 5)\tValid: X(293, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = ImageDataset(data_dir=data_dir)\n",
    "    data.create()  # creates features & label\n",
    "    data.save(save_file.format(data.size))  # saves this object\n",
    "    # data = data.load(save_file.format(data.size))  # loads saved object\n",
    "    \n",
    "    # Split into training, testing & validation set.\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = data.train_test_split(test_size=0.2, valid_portion=0.1)\n",
    "    # X_train, y_train, X_test, y_test = data.train_test_split(test_size=0.2)\n",
    "\n",
    "    print('\\nTrain: X{}\\tTest: y{}\\tValid: X{}'.format(X_train.shape, y_test.shape, X_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
